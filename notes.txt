20-7-19
I'm ready to program a mass backtester.

This program will test a binance-pybot strategy on multiple sets of historical price data.

I imagine it working like this:
- I will have a program called maba.py (mass backtester)
- maba.py has a class in it like binance-pybot's Instance class
- Inside the class will be a method for init and strat, which can be copied
from binance-pybot
- maba.py will be pointed to a data folder
- In the data folder will be several files containing historical price data
- Each file corresponds to a specific exchange, pair, timeframe, and period
- maba.py tests the strategy on each of the data files
- maba.py outputs the results to a text file

Then what I can do is tweak the strategy and retest it and compare how the tweak
affected the results. If I am running this test on a significant amount of
datasets then the effect of tweaking the strategy on the overall results is
more meaningful.

***

So here's the plan.
- First create a program that can get historical price data from Binance
- Get a couple of data files in the data folder
- Create the maba.py file and program it to do what I want

7-23
Notes:
- Use client.get_historical_klines
- Candles come back in a list and have this format:
[
  [
    1499040000000,      // Open time
    "0.01634790",       // Open
    "0.80000000",       // High
    "0.01575800",       // Low
    "0.01577100",       // Close
    "148976.11427815",  // Volume
    1499644799999,      // Close time
    "2434.19055334",    // Quote asset volume
    308,                // Number of trades
    "1756.87402397",    // Taker buy base asset volume
    "28.46694368",      // Taker buy quote asset volume
    "17928899.62484339" // Ignore.
  ]
]

More notes:
- If you initialize a datetime object with yyyy/mm/dd it gives you the start
of the day in your local time zone
- If you use the timestamp method, it gives unix time in seconds.
- If you use time.time, it gives the current unix time in seconds.
- But Binance days are based on UTC. So it would be preferable if the datetime
object initialized at the start of the day in UTC time
- Also, Binance takes timestamps in ms, not seconds, so multiply by 1000
    d = datetime.datetime(2020, 1, 1, tzinfo=datetime.timezone.utc)
    print(1000 * d.timestamp())

So basically what I can do is initialize a datetime object with a year, month,
and day. One for start_date, one for end_date. Then convert it to the format
needed, and retrieve klines directly using get_historical_klines.

Notes:
- I won't bother complicating it with time of day
- I really don't think it's necessary to have time of day precision

The program will take a pair, interval, and start/end date w/o time precision.
It start at the start-of-day in UTC time, and end at the end-of-day for the
start and end dates provided. And it will get the candles in that period of
the specified interval.
- Technically I could test one full day by putting the same date for start/end

***

I verified that my new method, putting yyyy/mm/dd instead of the strings, gives
the same result from Binance as using the strings.

***

Note: When a live instance starts up it gets 600 historical candles so it
can feed that data to talib and get indicators. So for the backtest to be
accurate it has to get 600 extra early candles too.

I'll leave the number of early candles for indicators as a variable in the
get_date code.


***

Note: If I get klines from Binance direcly I am limited by the tick offset that
I'm allowed to do and also by the intervals that I can do. For example, I can't
do daily candles in my local timezone, and I can't do 45 minute candles.

However, if I need to do something like that then I can always just get it some
other way. Ultimately once I have the data I should be able to use the
backtester on it regardless of the tick offset or interval.

For now I don't care about using an arbitrary tick offset or interval size.

If I did want to allow arbitrary tick offset or interval size, I would have
to import 1m candles for the whole range, then compile the custom candles.
That's over 500k candles for 1 year.

7-24
Okay I set it up so it gets exactly n_early_candles extra candles on the front,
for calculating indicators.

So the mass backtester will have to know to start on the 601th candle. Basically
run the first 601 candles. Then the first 601 candles excluding the very first
one. And so on.

7-30
Maba will be very similar to Pybot, because it's intended to mimic live Pybot
strategies for comparison. So I'll try to copy Pybot as much as possible.

Here's how it will work:
- Create a folder for a given strategy: strat_1/
- Add strat_1/data/ and strat_1/maba.py
- Replace strat and init in maba.py to strat_1 (strat_1_strat.py)
- Use get_data.py to populate data/ with some datasets
- Then run maba.py
- maba.py tests each dataset, and makes a log folder for each dataset
- maba.py creates a summary.txt file and gives me the results in my desired format

strat_1/
    maba_strat_1.py
    config.txt
    summary.txt
    data/
        ethbtc_2h_200101-200201.txt
        xrpbtc_2h_200101-200201.txt
    logs/
        ethbtc_2h_200101-200201/
            2001/
                200101.log
                ...
        xrpbtc_2h_200101-200201/
            2001/
                200101.log
                ...

Then when I'm comparing strategies I can just look at the summary.txt file
and get the relevant data and add it to my spreadsheet.

Notes:
- I don't have to connect to Binance API at all
- I just have to provide a path to the data, which will most likely be data/
- I also have to provide n_early_candles and make sure it matches the value from
get_data.py, so the backtester knows where to start, most likely 600

- I run maba.py
- It goes into data, gets the first data file
- Updates logger and makes a log file corresponding to the file name
- It runs through the data similar to how Pybot works, saving logs
- It stores whatever data it needs to compile the summary before moving on
to the next dataset
- After all datasets are complete, it refers to the stored data and creates
the summary.txt file

***

Pybot works by setting up logging then creating an Instance class. The Instance
class uses the pre-defined Binance client to get candles, get portfolio info,
do trading, calculate the trading algorithm etc. The Instance class stays
updated by pinging every 0.5 seconds.

Maba works by going into data/, and cycling through the the files. For each
file it sets up logging and creates a Backtest class. The Backtest class will
mimic the Instance class using the data.

***

I'm thinking I can store the backtest results in a list.
Like backtests[0] = Backtest().
That way I could easily retrieve any information I want when I'm trying to
create the summary.
But I want to avoid storing the price data for all of the datasets.
Really, the things that I need are:
- name
- ticks, days, trades
- performance

Maybe what I'll do is just create a custom object for each Backtest instance,
with all the info that I need. Save that to a list. Then use the list to
create the summary.

***

The Backtest object will take the path to the data file to initialize. From
this, it needs to get the asset, base, and interval_mins.
- With the way I'm doing it, it's hard to get the asset and the base. I should
be using currency pair in the naming scheme instead of pair.
- Also, it's hard to get interval_mins. I should be using interval_mins in the
naming scheme instead of interval.
