20-7-19
I'm ready to program a mass backtester.

This program will test a binance-pybot strategy on multiple sets of historical price data.

I imagine it working like this:
- I will have a program called maba.py (mass backtester)
- maba.py has a class in it like binance-pybot's Instance class
- Inside the class will be a method for init and strat, which can be copied
from binance-pybot
- maba.py will be pointed to a data folder
- In the data folder will be several files containing historical price data
- Each file corresponds to a specific exchange, pair, timeframe, and period
- maba.py tests the strategy on each of the data files
- maba.py outputs the results to a text file

Then what I can do is tweak the strategy and retest it and compare how the tweak
affected the results. If I am running this test on a significant amount of
datasets then the effect of tweaking the strategy on the overall results is
more meaningful.

***

So here's the plan.
- First create a program that can get historical price data from Binance
- Get a couple of data files in the data folder
- Create the maba.py file and program it to do what I want

7-23
Notes:
- Use client.get_historical_klines
- Candles come back in a list and have this format:
[
  [
    1499040000000,      // Open time
    "0.01634790",       // Open
    "0.80000000",       // High
    "0.01575800",       // Low
    "0.01577100",       // Close
    "148976.11427815",  // Volume
    1499644799999,      // Close time
    "2434.19055334",    // Quote asset volume
    308,                // Number of trades
    "1756.87402397",    // Taker buy base asset volume
    "28.46694368",      // Taker buy quote asset volume
    "17928899.62484339" // Ignore.
  ]
]

More notes:
- If you initialize a datetime object with yyyy/mm/dd it gives you the start
of the day in your local time zone
- If you use the timestamp method, it gives unix time in seconds.
- If you use time.time, it gives the current unix time in seconds.
- But Binance days are based on UTC. So it would be preferable if the datetime
object initialized at the start of the day in UTC time
- Also, Binance takes timestamps in ms, not seconds, so multiply by 1000
    d = datetime.datetime(2020, 1, 1, tzinfo=datetime.timezone.utc)
    print(1000 * d.timestamp())

So basically what I can do is initialize a datetime object with a year, month,
and day. One for start_date, one for end_date. Then convert it to the format
needed, and retrieve klines directly using get_historical_klines.

Notes:
- I won't bother complicating it with time of day
- I really don't think it's necessary to have time of day precision

The program will take a pair, interval, and start/end date w/o time precision.
It start at the start-of-day in UTC time, and end at the end-of-day for the
start and end dates provided. And it will get the candles in that period of
the specified interval.
- Technically I could test one full day by putting the same date for start/end

***

I verified that my new method, putting yyyy/mm/dd instead of the strings, gives
the same result from Binance as using the strings.

***

Note: When a live instance starts up it gets 600 historical candles so it
can feed that data to talib and get indicators. So for the backtest to be
accurate it has to get 600 extra early candles too.

I'll leave the number of early candles for indicators as a variable in the
get_date code.


***

Note: If I get klines from Binance direcly I am limited by the tick offset that
I'm allowed to do and also by the intervals that I can do. For example, I can't
do daily candles in my local timezone, and I can't do 45 minute candles.

However, if I need to do something like that then I can always just get it some
other way. Ultimately once I have the data I should be able to use the
backtester on it regardless of the tick offset or interval.

For now I don't care about using an arbitrary tick offset or interval size.

If I did want to allow arbitrary tick offset or interval size, I would have
to import 1m candles for the whole range, then compile the custom candles.
That's over 500k candles for 1 year.

7-24
Okay I set it up so it gets exactly n_early_candles extra candles on the front,
for calculating indicators.

So the mass backtester will have to know to start on the 601th candle. Basically
run the first 601 candles. Then the first 601 candles excluding the very first
one. And so on.

7-30
Maba will be very similar to Pybot, because it's intended to mimic live Pybot
strategies for comparison. So I'll try to copy Pybot as much as possible.

Here's how it will work:
- Create a folder for a given strategy: strat_1/
- Add strat_1/data/ and strat_1/maba.py
- Replace strat and init in maba.py to strat_1 (strat_1_strat.py)
- Use get_data.py to populate data/ with some datasets
- Then run maba.py
- maba.py tests each dataset, and makes a log folder for each dataset
- maba.py creates a summary.txt file and gives me the results in my desired format

strat_1/
    maba_strat_1.py
    config.txt
    summary.txt
    data/
        ethbtc_2h_200101-200201.txt
        xrpbtc_2h_200101-200201.txt
    logs/
        ethbtc_2h_200101-200201/
            2001/
                200101.log
                ...
        xrpbtc_2h_200101-200201/
            2001/
                200101.log
                ...

Then when I'm comparing strategies I can just look at the summary.txt file
and get the relevant data and add it to my spreadsheet.

Notes:
- I don't have to connect to Binance API at all
- I just have to provide a path to the data, which will most likely be data/
- I also have to provide n_early_candles and make sure it matches the value from
get_data.py, so the backtester knows where to start, most likely 600

- I run maba.py
- It goes into data, gets the first data file
- Updates logger and makes a log file corresponding to the file name
- It runs through the data similar to how Pybot works, saving logs
- It stores whatever data it needs to compile the summary before moving on
to the next dataset
- After all datasets are complete, it refers to the stored data and creates
the summary.txt file

***

Pybot works by setting up logging then creating an Instance class. The Instance
class uses the pre-defined Binance client to get candles, get portfolio info,
do trading, calculate the trading algorithm etc. The Instance class stays
updated by pinging every 0.5 seconds.

Maba works by going into data/, and cycling through the the files. For each
file it sets up logging and creates a Backtest class. The Backtest class will
mimic the Instance class using the data.

***

I'm thinking I can store the backtest results in a list.
Like backtests[0] = Backtest().
That way I could easily retrieve any information I want when I'm trying to
create the summary.
But I want to avoid storing the price data for all of the datasets.
Really, the things that I need are:
- name
- ticks, days, trades
- performance

Maybe what I'll do is just create a custom object for each Backtest instance,
with all the info that I need. Save that to a list. Then use the list to
create the summary.

***

The Backtest object will take the path to the data file to initialize. From
this, it needs to get the asset, base, and interval_mins.
- With the way I'm doing it, it's hard to get the asset and the base. I should
be using currency pair in the naming scheme instead of pair.
- Also, it's hard to get interval_mins. I should be using interval_mins in the
naming scheme instead of interval.

***

The set_log_file function sets the destination and the format for logs.
In pybot, it's called at the start, then at the start of each tick. The format
never changes, but the destination may change at the start of a tick.

In maba, the situation will be the same. But I need to also set the destination
at the start of analyzing a new data file. As if I were starting a new live
instance.

Also in maba the time will be calculated in a different way; based on the
candle data instead of getting the current time.

Notes:
- The Backtest object does all of the logging in logs/
- The only other logging is into summary.txt at the end
- Otherwise, messages will be printed to the console
- So I have to set up the logger:
    - Whenever initializing a new backtest instance
    - At the start of a new tick when backtesting
    - Before logging the summary

***

Notes:
- set_log_file for maba also needs to be given dsname (dataset name) because
it creates a folder of the same name inside logs/ to store data in
- set_log_file for pybot doesn't need dsname or a timestamp, because it doesn't
create the subfolder, and it gets timestamps from checking the actual time
- set_log_file won't work for setting up the summary. I will probably have to
create a custom function, possibly called set_summary_file

8-12
Notes:
- This assumes that there's no minimum order size. So if your real bots start
hitting minimum order limits, then the real results differ from backtest
results.
- This does not round the buy/sell amounts or the prices to number of
sig figs that Binance requires because it's not communicating with Binance and
it can't get that information. Again, the smaller your real account size, the
more this matters. With a large volume, rounding will be negligible.
- Therefore it's important to give your bots sufficient funds. Because with a
small account size they will differ more from backtesting.
- However, it will round to 8 sig figs, since that's the maximum Binance
would allow in general.

8-13
To do list:
/- Add strat and bso
/- Make sure the backtester is simulating trading
- Go back and figure out how to implement pfake and ptrade so the log is accurate

8-14
Again had an issue with copying positions.

Remember, when you copy a dict like dict2 = dict(dict1) it copies the structure
but the objects within dict1 are not copied.

So in my case there is a list within dict1, I'm creating dict2 then changing
an item within the list, which is then changing in dict2, even though I thought
dict2 was supposed to be a new object. It's because the two dict objects are
pointing to the same internal list. I need to deep clone a dict. Maybe it's not
necessary if the key/value pairs are just strings or numbers. But it's true
if they contain lists or other dicts.

In Binance Pybot in the get_positions function I was not cloning self.positions.
I was explicitly defining the new object and then getting the positions from
Binance API.

To deep copy a dict it's often necessary to import the copy module. Then go
copy.deepcopy(dict1).

But you can avoid doing this by being creative. For example, by explicitly
cloning the internal objects.

In Binance Pybot I should probably remove all instances where I shallow copy
a dict which needs to be deep copied. Just for completeness. Even though it's
currently working by chance.

I don't think it's necessary for me to import the copy module in these cases.

8-18
For the summary I have to:
- set the logger to the summary file
- cycle through the datasets and log the relevant data

***

How to use:
- Create a new mass-backtester folder for each strategy to be tested, rename
as appropriate
- You can remove readme, license, notes to clear up space
- Copy over the strategy file and replace the code in the maba.py file
- Get the datasets to be tested using get_data.py
- Copy the datasets to each mass-backtester folder
- Program maba.py to output the data that you are interested in
- Run maba.py
- maba.py creates a summary file in the format you specified
- Now you can compare each of the strategies by whatever metrics you want by
entering the corresponding maba folders and checking the results of the summary
files. You can enter the results into a spreadsheet and do more comparisons.

Notes:
- Make sure n_early_candles is the same in maba.py and get_data.py before you
retrieve the datasets
